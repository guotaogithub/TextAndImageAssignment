import numpy as np
import torch
import matplotlib.pyplot as plt
import seaborn as sns
import warnings

from DataConfig import DataConfig
from ModelTrainer import ModelTrainer
from MultimodalDataLoader import MultimodalDataLoader


# é…ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
def setup_chinese_font():
    """é…ç½®matplotlibä¸­æ–‡å­—ä½“æ”¯æŒ"""
    chinese_fonts = [
        'PingFang SC', 'Hiragino Sans GB', 'Arial Unicode MS',
        'SimHei', 'Microsoft YaHei', 'WenQuanYi Micro Hei'
    ]
    plt.rcParams['font.sans-serif'] = chinese_fonts
    plt.rcParams['axes.unicode_minus'] = False
    warnings.filterwarnings('ignore', category=UserWarning, module='matplotlib.font_manager')

setup_chinese_font()



# ======================== ä¸»ç¨‹åº ========================
def main():
    """ä¸»ç¨‹åº"""
    print("ğŸ¯ å¤šæ¨¡æ€è°è¨€æ£€æµ‹ç³»ç»Ÿ")
    print("=" * 60)

    # 1. é…ç½®å’Œæ£€æŸ¥æ•°æ®è·¯å¾„
    config = DataConfig()
    if not config.check_paths():
        print("âŒ æ•°æ®è·¯å¾„æ£€æŸ¥å¤±è´¥ï¼Œè¯·ç¡®ä¿æ‰€æœ‰æ•°æ®æ–‡ä»¶å­˜åœ¨")
        return

    # 2. åŠ è½½å¤šæ¨¡æ€æ•°æ®
    data_loader = MultimodalDataLoader(config)
    data_result = data_loader.load_all_data()

    if data_result is None:
        print("âŒ æ•°æ®åŠ è½½å¤±è´¥")
        return

    # ä»å­—å…¸ä¸­æå–æ•°æ®
    audio_features = data_result.get('audio_features')
    text_features = data_result.get('text_features')
    visual_features = data_result.get('visual_features')
    annotation_features = data_result.get('annotation_features')

    # æå–æ ‡ç­¾ - ä½¿ç”¨æœ€å®Œæ•´çš„æ ‡ç­¾é›†
    audio_labels = data_result.get('audio_labels')
    text_labels = data_result.get('text_labels')
    visual_labels = data_result.get('visual_labels')
    annotation_labels = data_result.get('annotation_labels')

    # é€‰æ‹©æœ€å¤§çš„æ ‡ç­¾é›†ä½œä¸ºä¸»è¦æ ‡ç­¾
    all_labels = []
    for label_set in [audio_labels, text_labels, visual_labels, annotation_labels]:
        if label_set is not None:
            all_labels.append(label_set)

    if not all_labels:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ ‡ç­¾")
        return

    # ä½¿ç”¨æœ€é•¿çš„æ ‡ç­¾é›†
    labels = max(all_labels, key=len)
    print(f"ğŸ“Š ä½¿ç”¨æ ‡ç­¾é›†ï¼Œå¤§å°: {len(labels)}")

    # æ£€æŸ¥æ˜¯å¦æœ‰è‡³å°‘ä¸€ç§æ¨¡æ€çš„æ•°æ®
    available_modalities = []
    if audio_features is not None:
        available_modalities.append('audio')
    if text_features is not None:
        available_modalities.append('text')
    if visual_features is not None:
        available_modalities.append('visual')
    if annotation_features is not None:
        available_modalities.append('annotation')

    if not available_modalities:
        print("âŒ æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•æ¨¡æ€çš„ç‰¹å¾")
        return

    print(f"âœ… æˆåŠŸåŠ è½½æ¨¡æ€: {available_modalities}")

    # 3. è®­ç»ƒä¸åŒèåˆç­–ç•¥çš„æ¨¡å‹
    trainer = ModelTrainer()

    # è®­ç»ƒæ‹¼æ¥èåˆæ¨¡å‹
    print("\n" + "=" * 60)
    print("ğŸ¤– å¼€å§‹è®­ç»ƒæ‹¼æ¥èåˆæ¨¡å‹...")
    try:
        concat_model, concat_scalers = trainer.train_model(
            audio_features, text_features, visual_features, annotation_features, labels,
            fusion_type='concat', epochs=80, batch_size=16
        )
        print("âœ… æ‹¼æ¥èåˆæ¨¡å‹è®­ç»ƒå®Œæˆ")
    except Exception as e:
        print(f"âŒ æ‹¼æ¥èåˆæ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
        concat_model, concat_scalers = None, None

    # è®­ç»ƒæ³¨æ„åŠ›èåˆæ¨¡å‹
    print("\n" + "=" * 60)
    print("ğŸ¤– å¼€å§‹è®­ç»ƒæ³¨æ„åŠ›èåˆæ¨¡å‹...")
    try:
        attention_model, attention_scalers = trainer.train_model(
            audio_features, text_features, visual_features, annotation_features, labels,
            fusion_type='attention', epochs=80, batch_size=16
        )
        print("âœ… æ³¨æ„åŠ›èåˆæ¨¡å‹è®­ç»ƒå®Œæˆ")
    except Exception as e:
        print(f"âŒ æ³¨æ„åŠ›èåˆæ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
        attention_model, attention_scalers = None, None

    # è®­ç»ƒåŠ æƒèåˆæ¨¡å‹
    print("\n" + "=" * 60)
    print("ğŸ¤– å¼€å§‹è®­ç»ƒåŠ æƒèåˆæ¨¡å‹...")
    try:
        weighted_model, weighted_scalers = trainer.train_model(
            audio_features, text_features, visual_features, annotation_features, labels,
            fusion_type='weighted', epochs=80, batch_size=16
        )
        print("âœ… åŠ æƒèåˆæ¨¡å‹è®­ç»ƒå®Œæˆ")
    except Exception as e:
        print(f"âŒ åŠ æƒèåˆæ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
        weighted_model, weighted_scalers = None, None

    # 4. ä¿å­˜æ¨¡å‹
    print("\nğŸ’¾ ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹...")
    saved_models = []

    if concat_model is not None:
        torch.save(concat_model.state_dict(), 'concat_fusion_model.pth')
        saved_models.append('concat')

    if attention_model is not None:
        torch.save(attention_model.state_dict(), 'attention_fusion_model.pth')
        saved_models.append('attention')

    if weighted_model is not None:
        torch.save(weighted_model.state_dict(), 'weighted_fusion_model.pth')
        saved_models.append('weighted')

    # ä¿å­˜ç‰¹å¾ç¼©æ”¾å™¨
    import pickle
    if concat_scalers is not None:
        with open('concat_scalers.pkl', 'wb') as f:
            pickle.dump(concat_scalers, f)
    if attention_scalers is not None:
        with open('attention_scalers.pkl', 'wb') as f:
            pickle.dump(attention_scalers, f)
    if weighted_scalers is not None:
        with open('weighted_scalers.pkl', 'wb') as f:
            pickle.dump(weighted_scalers, f)

    print(f"âœ… æ¨¡å‹ä¿å­˜å®Œæˆï¼æˆåŠŸä¿å­˜çš„æ¨¡å‹: {saved_models}")

    # 5. ç‰¹å¾é‡è¦æ€§åˆ†æ
    print("\nğŸ“Š ç”Ÿæˆç‰¹å¾åˆ†æå›¾è¡¨...")
    try:
        analyze_multimodal_features(audio_features, text_features, visual_features, annotation_features, labels)
        print("âœ… ç‰¹å¾åˆ†æå®Œæˆ")
    except Exception as e:
        print(f"âŒ ç‰¹å¾åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


def analyze_multimodal_features(audio_features, text_features, visual_features, annotation_features, labels):
    """å¤šæ¨¡æ€ç‰¹å¾åˆ†æ"""

    # 1. å„æ¨¡æ€ç‰¹å¾é‡è¦æ€§
    fig, axes = plt.subplots(2, 2, figsize=(20, 15))

    # éŸ³é¢‘ç‰¹å¾é‡è¦æ€§
    audio_importance = np.var(audio_features, axis=0)
    top_audio_idx = np.argsort(audio_importance)[::-1][:15]

    axes[0, 0].bar(range(len(top_audio_idx)), audio_importance[top_audio_idx])
    axes[0, 0].set_title('éŸ³é¢‘ç‰¹å¾é‡è¦æ€§ (Top 15)')
    axes[0, 0].set_xlabel('ç‰¹å¾ç´¢å¼•')
    axes[0, 0].set_ylabel('é‡è¦æ€§åˆ†æ•°')

    # æ–‡æœ¬ç‰¹å¾é‡è¦æ€§
    text_importance = np.var(text_features, axis=0)
    top_text_idx = np.argsort(text_importance)[::-1][:15]

    axes[0, 1].bar(range(len(top_text_idx)), text_importance[top_text_idx])
    axes[0, 1].set_title('æ–‡æœ¬ç‰¹å¾é‡è¦æ€§ (Top 15)')
    axes[0, 1].set_xlabel('ç‰¹å¾ç´¢å¼•')
    axes[0, 1].set_ylabel('é‡è¦æ€§åˆ†æ•°')

    # è§†è§‰ç‰¹å¾é‡è¦æ€§
    visual_importance = np.var(visual_features, axis=0)
    top_visual_idx = np.argsort(visual_importance)[::-1][:15]

    axes[1, 0].bar(range(len(top_visual_idx)), visual_importance[top_visual_idx])
    axes[1, 0].set_title('è§†è§‰ç‰¹å¾é‡è¦æ€§ (Top 15)')
    axes[1, 0].set_xlabel('ç‰¹å¾ç´¢å¼•')
    axes[1, 0].set_ylabel('é‡è¦æ€§åˆ†æ•°')

    # æ ‡æ³¨ç‰¹å¾é‡è¦æ€§
    annotation_importance = np.var(annotation_features, axis=0)
    top_annotation_idx = np.argsort(annotation_importance)[::-1][:15]

    axes[1, 1].bar(range(len(top_annotation_idx)), annotation_importance[top_annotation_idx])
    axes[1, 1].set_title('æ ‡æ³¨ç‰¹å¾é‡è¦æ€§ (Top 15)')
    axes[1, 1].set_xlabel('ç‰¹å¾ç´¢å¼•')
    axes[1, 1].set_ylabel('é‡è¦æ€§åˆ†æ•°')

    plt.tight_layout()
    plt.show()

    # 2. æ¨¡æ€é—´ç›¸å…³æ€§åˆ†æ
    plt.figure(figsize=(12, 8))

    # è®¡ç®—å„æ¨¡æ€çš„å¹³å‡ç‰¹å¾
    audio_mean = np.mean(audio_features, axis=1)
    text_mean = np.mean(text_features, axis=1)
    visual_mean = np.mean(visual_features, axis=1)
    annotation_mean = np.mean(annotation_features, axis=1)

    modality_data = np.column_stack([audio_mean, text_mean, visual_mean, annotation_mean])
    correlation_matrix = np.corrcoef(modality_data.T)

    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,
                xticklabels=['éŸ³é¢‘', 'æ–‡æœ¬', 'è§†è§‰', 'æ ‡æ³¨'],
                yticklabels=['éŸ³é¢‘', 'æ–‡æœ¬', 'è§†è§‰', 'æ ‡æ³¨'])
    plt.title('å¤šæ¨¡æ€ç‰¹å¾ç›¸å…³æ€§åˆ†æ')
    plt.show()

    # 3. ç±»åˆ«åˆ†å¸ƒå¯è§†åŒ–
    plt.figure(figsize=(15, 5))

    # å„æ¨¡æ€åœ¨ä¸åŒç±»åˆ«ä¸‹çš„åˆ†å¸ƒ
    modalities = ['éŸ³é¢‘', 'æ–‡æœ¬', 'è§†è§‰', 'æ ‡æ³¨']
    features_list = [audio_features, text_features, visual_features, annotation_features]

    for i, (modality, features) in enumerate(zip(modalities, features_list)):
        plt.subplot(1, 4, i + 1)

        true_features = features[labels == 0]
        false_features = features[labels == 1]

        plt.hist(np.mean(true_features, axis=1), alpha=0.7, label='çœŸè¯', bins=20)
        plt.hist(np.mean(false_features, axis=1), alpha=0.7, label='å‡è¯', bins=20)

        plt.title(f'{modality}ç‰¹å¾åˆ†å¸ƒ')
        plt.xlabel('ç‰¹å¾å€¼')
        plt.ylabel('é¢‘æ¬¡')
        plt.legend()

    plt.tight_layout()
    plt.show()


if __name__ == "__main__":
    main()